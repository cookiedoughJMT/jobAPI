from fastapi import UploadFile, File
from fastapi import APIRouter
from dotenv import load_dotenv

import os, json, numpy as np

import whisper
from pyAudioAnalysis import audioBasicIO, ShortTermFeatures
from openai import OpenAI

from pydub import AudioSegment
from pydub.utils import which

import logging
import re

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê³µí†µ ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

ffmpeg_path = r"C:\ffmpeg\bin\ffmpeg.exe"

# í™˜ê²½ë³€ìˆ˜ ì¶”ê°€
os.environ["PATH"] += os.pathsep + os.path.dirname(ffmpeg_path)

# pydubì— ëª…ì‹œì  ì§€ì •
AudioSegment.converter = ffmpeg_path

# pydub ë‚´ë¶€ ê²½ë¡œì—ë„ ì„¤ì • (ì¶”ê°€ ë°©ì–´)
if not which("ffmpeg"):
    print("âŒ ffmpeg ê²½ë¡œ í™•ì¸ ì‹¤íŒ¨")
else:
    print("âœ… ffmpeg ê²½ë¡œ í™•ì¸ë¨:", which("ffmpeg"))

# FastAPI ì•±
audio_api = APIRouter()

# whisper ëª¨ë¸ (1íšŒë§Œ ë¡œë“œ)
whisper_model = whisper.load_model("base")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘  ìŒì„± ë¶„ì„ ë¼ìš°í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# audio_api = APIRouter(prefix="/api/audio")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í—¬í¼ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# pitch contour êµ¬í•˜ê¸° librosa(ìŒì„±ë¶„ì„ë¼ì´ë¸ŒëŸ¬ë¦¬)
def compute_pitch_contour(signal, fs, win, step):
    import librosa
    pitches, _ = librosa.piptrack(y=signal, sr=fs, n_fft=win, hop_length=step)
    pitch_contour = []
    for i in range(pitches.shape[1]):
        pitch_values = pitches[:, i]
        max_pitch = pitch_values.max()
        pitch_contour.append(max_pitch if max_pitch > 0 else 0)
    return pitch_contour

def extract_tone_pattern_from_pitch(pitch_series):
    result, length = [], len(pitch_series)
    step = max(1, length // 5)
    for i in range(0, length, step):
        segment = pitch_series[i:i + step]
        level = min(100, int(np.std(segment) * 1000))  # ë³€ë™ì„± â†’ 0~100
        result.append({"position": int(i / length * 100), "level": level})
    return result

def clean_md_json(text: str) -> str:
    """ ```json ... ```  ê°ì‹¸ê¸° ì œê±° """
    return re.sub(r"^```json\s*|\s*```$", "", text.strip(), flags=re.DOTALL)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@audio_api.post("/analyze")
async def analyze_audio(file: UploadFile = File(...)):
    try:
        # 0) ì—…ë¡œë“œ ì €ì¥ & ë³€í™˜ -------------------------------------------------
        raw = await file.read()
        with open("uploaded.webm", "wb") as f:
            f.write(raw)

        AudioSegment.from_file("uploaded.webm").export("uploaded.wav", format="wav")

        # 1) Whisper -----------------------------------------------------------
        transcript = whisper_model.transcribe("uploaded.wav")["text"]

        # 2) pyAudioAnalysis ---------------------------------------------------
        Fs, x = audioBasicIO.read_audio_file("uploaded.wav")
        x = audioBasicIO.stereo_to_mono(x)

        # â­ï¸ librosa ì‚¬ìš©ì„ ìœ„í•´ float32ë¡œ ë³€í™˜
        x = x.astype(np.float32)

        if np.mean(x ** 2) < 1e-5:
            return {"error": "ë¬´ìŒì´ê±°ë‚˜ ë„ˆë¬´ ì‘ìŒ"}

        win = step = int(0.05 * Fs)
        F, f_names = ShortTermFeatures.feature_extraction(x, Fs, win, step)

        # print("ğŸ§ f_names í™•ì¸:", f_names)


        if F.shape[1] == 0:
            return {"error": "ìœ íš¨í•œ í”„ë ˆì„ ì—†ìŒ"}

        features = {k: float(np.mean(v)) for k, v in zip(f_names, F)}
        feat_txt = "\n".join(f"- {k}: {v:.4f}" for k, v in features.items())

        # 3) GPT í”„ë¡¬í”„íŠ¸ -------------------------------------------------------
        prompt = f"""
        ë‹¤ìŒì€ í•œ ì‚¬ìš©ìì˜ ìŒì„± ë©´ì ‘ ë°ì´í„°ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸ì™€ ìŒì„± í”¼ì²˜ë¥¼ ì°¸ê³ í•´ ì•„ë˜ JSON ìŠ¤í‚¤ë§ˆì— **ë”± ë§ì¶°ì„œ** (ë°±í‹±Â·ì£¼ì„ ì—†ì´) ì‘ë‹µí•˜ì„¸ìš”.
        speedPattern ì˜ dataê°’ì€ ìµœëŒ€í•œ ì—¬ëŸ¬ê°œë¥¼ ë½‘ì•„ì£¼ì‹œê³  position ì˜ ê°’ì€ ìµœì†Œ 10ì”© ì°¨ì´ë‚˜ê²Œ í•´ì£¼ì„¸ìš”.
        strengths, improvements, improvementStrategies ì´ í•­ëª©ì€ descriptionì— ë‹´ì•„ì•¼í•  ë‚´ìš©ì˜ ì£¼ì œë¥¼ ì„¤ëª…í–ˆìŠµë‹ˆë‹¤.
        ì°¸ê³ í•˜ì—¬ ìì„¸íˆ ì‘ì„±í•´ì£¼ì„¸ìš”.
        
        ğŸ—£ í…ìŠ¤íŠ¸
        \"\"\"{transcript}\"\"\"
        
        ğŸ§ ìŒì„± í”¼ì²˜ ìš”ì•½
        {feat_txt}
        
        {{
          "overallScore":  (10~100 ì •ìˆ˜),
          "clarity":       (0~100 ì •ìˆ˜),
          "speed":         (0~100 ì •ìˆ˜),
          "volume":        (0~100 ì •ìˆ˜),
          "confidence":    (overallScore ë™ì¼),
        
          "speechMetrics": {{
            "wordsPerMinute": 0,
            "clarity":        0,
            "intonation":     0,
            "pauseDuration":  0.0,
            "pronunciation":  0,
            "fillers":        0
          }},
        
          "voicePatterns": {{
            "volumePattern": {{
              "description": "ë¬¸ì¥",
              "data": [0,0,0,0,0]
            }},
            "speedPattern": {{
              "description": "ë¬¸ì¥",
              "data": [{{"position":0,"level":0}},...]
            }},
            "tonePattern": {{
              "description": "ë¬¸ì¥"
            }}
          }},
        
         "strengths": [
            {{ "title": "ì§§ì€ ì œëª©(ê°•ì )", "description": "ì „ì²´ ê°•ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”" }},
            {{ "title": "ì§§ì€ ì œëª©(ê°•ì )", "description": "ë””í…Œì¼í•œ ê°•ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”" }}, ...
          ],
          "improvements": [
            {{ "title": "ì§§ì€ ì œëª©(ê°œì„ ì )", "description": "ì „ì²´ ê°œì„ ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”" }},
            {{ "title": "ì§§ì€ ì œëª©(ê°œì„ ì )", "description": " ë””í…Œì¼í•œ ê°œì„ ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”" }}, ...
          ],
          "improvementStrategies": [
            {{ "title": "ì§§ì€ ì œëª©(ê°œì„  ì „ëµ)", "description": "í•œ ë¬¸ì¥ ì„¤ëª…" }}, ...
          ]
        }}
        
        ì§€ì¹¨
        - ìœ„ JSON í‚¤Â·êµ¬ì¡° ë³€í˜• ê¸ˆì§€, ìˆ«ìëŠ” ì •ìˆ˜ ë˜ëŠ” ì†Œìˆ˜ 1ìë¦¬.
        """

        gpt_resp = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
        )

        # 4) GPT ì‘ë‹µ íŒŒì‹± ------------------------------------------------------
        cleaned = clean_md_json(gpt_resp.choices[0].message.content)
        parsed = json.loads(cleaned)          # dict

        # 5) tonePattern.data ì§ì ‘ ê³„ì‚° í›„ ì‚½ì… -------------------------------
        pitch_series = compute_pitch_contour(x, Fs, win, step)
        tone_pattern = extract_tone_pattern_from_pitch(pitch_series)

        if "voicePatterns" in parsed and "tonePattern" in parsed["voicePatterns"]:
            parsed["voicePatterns"]["tonePattern"]["data"] = tone_pattern
        else:
            parsed["voicePatterns"]["tonePattern"] = {
                "description": "ì‘ë‹µ ë‚´ tonePattern ëˆ„ë½ë¨, ì§ì ‘ ì¶”ê°€ë¨",
                "data": tone_pattern
            }

        # 6) ìµœì¢… ë°˜í™˜ ----------------------------------------------------------
        return {
            "transcript": transcript,
            "features": features,
            "gpt_feedback": parsed     # â† ì´ë¯¸ dict!
        }

    except Exception as e:
        logging.exception("analyze_audio error")
        return {"error": str(e)}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# uvicorn main:app --reload
