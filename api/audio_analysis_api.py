from fastapi import UploadFile, File
from fastapi import APIRouter
from dotenv import load_dotenv
from datetime import datetime

import os, json, numpy as np

import whisper
from pyAudioAnalysis import audioBasicIO, ShortTermFeatures
from openai import OpenAI

import time

import logging
import re

from pathlib import Path

import ffmpeg

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê³µí†µ ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI"))

# ffmpeg_path = r"C:\ffmpeg\bin\ffmpeg.exe"

# í™˜ê²½ë³€ìˆ˜ ì¶”ê°€
# os.environ["PATH"] += os.pathsep + os.path.dirname(ffmpeg_path)

# pydubì— ëª…ì‹œì  ì§€ì •
# AudioSegment.converter = ffmpeg_path

# pydub ë‚´ë¶€ ê²½ë¡œì—ë„ ì„¤ì • (ì¶”ê°€ ë°©ì–´)
# if not which("ffmpeg"):
#     print("âŒ ffmpeg ê²½ë¡œ í™•ì¸ ì‹¤íŒ¨")
# else:
#     print("âœ… ffmpeg ê²½ë¡œ í™•ì¸ë¨:", which("ffmpeg"))



def convert_webm_to_wav(input_path, output_path):
    (
        ffmpeg
        .input(str(input_path))
        .output(str(output_path), format='wav', acodec='pcm_s16le', ac=1, ar='16000')  # 16kHz mono
        .overwrite_output()
        .run(quiet=True)
    )

# FastAPI ì•±
audio_api = APIRouter()

# whisper ëª¨ë¸ (1íšŒë§Œ ë¡œë“œ)
whisper_model = whisper.load_model("base")

# ë£¨íŠ¸ í”„ë¡œì íŠ¸ ê²½ë¡œ(ì˜ˆ: main.py ì˜†) ê¸°ì¤€ìœ¼ë¡œ í•˜ìœ„ í´ë” ì§€ì •
BASE_DIR       = Path(__file__).resolve().parent
WEBM_DIR       = BASE_DIR / "data" / "webm"
WAV_DIR        = BASE_DIR / "data" / "wav"

# í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
WEBM_DIR.mkdir(parents=True, exist_ok=True)
WAV_DIR.mkdir(parents=True,  exist_ok=True)

audio_api = APIRouter()
whisper_model = whisper.load_model("base")

def get_timestamp_name() -> str:
    """YYYYMMDD_HHMMSS í˜•íƒœì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ë°˜í™˜"""
    return datetime.now().strftime("%Y%m%d_%H%M%S")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘  ìŒì„± ë¶„ì„ ë¼ìš°í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# audio_api = APIRouter(prefix="/api/audio")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í—¬í¼ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# pitch contour êµ¬í•˜ê¸° librosa(ìŒì„±ë¶„ì„ë¼ì´ë¸ŒëŸ¬ë¦¬)
def compute_pitch_contour(x, Fs, win, step):
    import librosa

    # librosa.yinì€ í”„ë ˆì„ ë‹¨ìœ„ pitch ì¶”ì • â†’ window/step ì‚¬ì´ì¦ˆì— ë§ê²Œ hop ì„¤ì •
    pitches = librosa.yin(y=x, fmin=50, fmax=300, sr=Fs, frame_length=win, hop_length=step)

    # 0 ë˜ëŠ” ìŒìˆ˜ ê°’ ì œê±° (ìŒì„± ì—†ëŠ” êµ¬ê°„ì—ì„œ -1 ë˜ëŠ” 0 ë‚˜ì˜¬ ìˆ˜ ìˆìŒ)
    pitch_contour = [float(p) if p > 0 else 0.0 for p in pitches]

    return pitch_contour

def extract_tone_pattern_from_pitch(pitch_series):
    result, length = [], len(pitch_series)
    step = max(1, length // 5)
    for i in range(0, length, step):
        segment = pitch_series[i:i + step]
        level = min(100, int(np.std(segment) * 1000))  # ë³€ë™ì„± â†’ 0~100
        result.append({"position": int(i / length * 100), "level": level})
    return result

def clean_md_json(text: str) -> str:
    """ ```json ... ```  ê°ì‹¸ê¸° ì œê±° """
    return re.sub(r"^```json\s*|\s*```$", "", text.strip(), flags=re.DOTALL)

def compute_wpm_timeline(words, default_segment_size=5.0, short_segment_size=3.0, threshold=30.0):
    timeline = []

    if not words:
        return []

    current_start = 0.0
    end_time = words[-1]["end"]

    # ì¡°ê±´ ë¶„ê¸°: 10ì´ˆ ë¯¸ë§Œì´ë©´ 1ì´ˆ, 30ì´ˆ ë¯¸ë§Œì´ë©´ 3ì´ˆ, ì•„ë‹ˆë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ ê¸°ì¤€ ì‚¼ì•„ ì¶”ì¶œ
    if end_time < 10:
        segment_size = 1.0
    elif end_time < threshold:
        segment_size = short_segment_size
    else:
        segment_size = default_segment_size


    print('segment_size: ', segment_size)

    while current_start < end_time:
        current_end = current_start + segment_size
        count = sum(1 for w in words if current_start <= w["start"] < current_end)
        wpm = int(count / segment_size * 60)
        timeline.append({"time": round(current_start), "wpm": wpm})
        current_start += segment_size

    return timeline


def convert_wpm_timeline_to_speed_pattern(wpm_timeline):
    if not wpm_timeline:
        return []

    end_time = wpm_timeline[-1]["time"] + 5.0  # ë§ˆì§€ë§‰ êµ¬ê°„ í¬í•¨

    pattern = []
    for item in wpm_timeline:
        position = int((item["time"] / end_time) * 100)  # 0~100%
        level = item["wpm"]  # ì •ê·œí™” ì—†ì´ ì‹¤ì œ wpm ê°’ ê·¸ëŒ€ë¡œ
        pattern.append({ "position": position, "level": level })

    return pattern

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@audio_api.post("/analyze")
async def analyze_audio(file: UploadFile = File(...)):
    start_time = time.perf_counter()  # ì‹œì‘ ì‹œê°„

    try:
        # 0) ì—…ë¡œë“œ ì €ì¥ & ë³€í™˜ ---------------------------------------------------
        raw = await file.read()

        ts = get_timestamp_name()
        webm_fn = f"{ts}_upload.webm"
        wav_fn = f"{ts}_upload.wav"

        webm_path = WEBM_DIR / webm_fn
        wav_path = WAV_DIR / wav_fn

        # WebM ì €ì¥
        with open(webm_path, "wb") as f:
            f.write(raw)

        # WebM â†’ WAV ë³€í™˜
        # AudioSegment.from_file(webm_path).export(wav_path, format="wav")

        convert_webm_to_wav(webm_path, wav_path)

        # 1) Whisper ---------------------------------------------------
        whisper_result = whisper_model.transcribe(
            str(wav_path), word_timestamps=True, language="ko"
        )
        transcript = whisper_result["text"]

        if len(transcript) > 500:
            transcript = transcript[:500] + "..."

        # ë‹¨ì–´ ë‹¨ìœ„ ì‹œê°„ ì •ë³´ ì¶”ì¶œ
        words = []
        for seg in whisper_result.get("segments", []):
            words.extend(seg.get("words", []))

        # 2) pyAudioAnalysis ---------------------------------------------------
        Fs, x = audioBasicIO.read_audio_file(str(wav_path))
        x = audioBasicIO.stereo_to_mono(x)

        # â­ï¸ librosa ì‚¬ìš©ì„ ìœ„í•´ float32ë¡œ ë³€í™˜
        x = x.astype(np.float32)

        if np.mean(x ** 2) < 1e-5:
            return {"error": "ë¬´ìŒì´ê±°ë‚˜ ë„ˆë¬´ ì‘ìŒ"}

        win = step = int(0.05 * Fs)
        F, f_names = ShortTermFeatures.feature_extraction(x, Fs, win, step)

        # print("ğŸ§ f_names í™•ì¸:", f_names)


        if F.shape[1] == 0:
            return {"error": "ìœ íš¨í•œ í”„ë ˆì„ ì—†ìŒ"}

        features = {k: float(np.mean(v)) for k, v in zip(f_names, F)}
        feat_txt = "\n".join(f"- {k}: {v:.4f}" for k, v in features.items())

        print('transcript: ', transcript)
        wpm_timeline = compute_wpm_timeline(words)
        speed_pattern_data = convert_wpm_timeline_to_speed_pattern(wpm_timeline)
        print('speed_pattern_data: ', speed_pattern_data)

        # 3) GPT í”„ë¡¬í”„íŠ¸ -------------------------------------------------------
        prompt = f"""
        ë‹¤ìŒì€ í•œ ì‚¬ìš©ìì˜ ìŒì„± ë©´ì ‘ ë°ì´í„°ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸ì™€ ìŒì„± í”¼ì²˜ë¥¼ ì°¸ê³ í•´ ì•„ë˜ JSON ìŠ¤í‚¤ë§ˆì— **ë”± ë§ì¶°ì„œ** (ë°±í‹±Â·ì£¼ì„ ì—†ì´) ì‘ë‹µí•˜ì„¸ìš”.

        â—ï¸í•µì‹¬ ì˜ë¬´ ì‚¬í•­
        1. strengths / improvements / improvementStrategies í•­ëª©ì— ìµœì†Œ 4ê°œì”©, ê°€ëŠ¥í•˜ë©´ 8ê°œ ì´ìƒ ì‘ì„±.
        2. ì œëª©(title)ì€ ì„œë¡œ ë‹¤ë¥¸ ê´€ì (ë…¼ë¦¬Â·ì „ë¬¸ì„±Â·ìì‹ ê°Â·ì–´ì¡°Â·ì†ë„Â·ë§íˆ¬Â·ê³µê°ë ¥Â·ì‹œê°„ ê´€ë¦¬ ë“±)ìœ¼ë¡œ ë‹¤ì–‘í™”.
        3. description ì—ëŠ” êµ¬ì²´ì  ì‚¬ë¡€ë¥¼ í¬í•¨.
        4. improvements ì™€ improvementStrategies ëŠ” 1:1ë¡œ ëŒ€ì‘(ê°™ì€ ìˆœì„œ).

        í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ê±°ë‚˜ ì„±ì˜ê°€ ë¶€ì¡±í•˜ë©´ confidence, overallScore ë¥¼ 10~30 ì˜ì—­ìœ¼ë¡œ ë‚®ì¶° ì£¼ì„¸ìš”.
                
        strengths í•­ëª©ì€ ìµœì†Œ 4ê°œ ì´ìƒ í•´ì£¼ì„¸ìš”.
        improvements í•­ëª©ì€ ìµœì†Œ 4ê°œ ì´ìƒ í•´ì£¼ì„¸ìš”.
        improvementStrategies í•­ëª©ì€ ìµœì†Œ 4ê°œ ì´ìƒ í•´ì£¼ì„¸ìš”.                
                
        ì¤‘ë³µ ê¸ˆì§€:
        - strengths.title ê³¼ improvements.title ì€ ì ˆëŒ€ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ ì‘ì„±í•˜ì„¸ìš”.
          (ì¤‘ë³µì´ ìƒê¸¸ ê²½ìš°, improvements ìª½ í•­ëª©ì„ ì œê±°)
        
        ê·¼ê±° í•„ìˆ˜:
        - ëª¨ë“  description ì—ì„œëŠ” ì‹¤ì œ ìŒì„±Â·í…ìŠ¤íŠ¸ì—ì„œ í™•ì¸ ê°€ëŠ¥í•œ êµ¬ì²´ì  ê·¼ê±°ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.
          ê·¼ê±° ì—†ì´ ì¶”ìƒì  ì¹­ì°¬(ì˜ˆ: â€˜ì„±ì‹¤í•¨â€™, â€˜ê³µê°ì„± ë†’ìŒâ€™)ì€ ê¸ˆì§€í•©ë‹ˆë‹¤.
          
        ë¬¸ë§¥ë§Œ ë‚œí•´í•  ê²½ìš°ì—” ì˜ë¯¸ë¥¼ ë³´ì •Â·ì¶”ë¡ í•´ ìµœëŒ€í•œ ë„ˆê·¸ëŸ½ê²Œ í‰ê°€í•©ë‹ˆë‹¤.

        ğŸ—£ í…ìŠ¤íŠ¸
        \"\"\"{transcript}\"\"\"

        ğŸ§ ìŒì„± í”¼ì²˜ ìš”ì•½
        {feat_txt}
        - êµ¬ê°„ë³„ ì†ë„ ë³€í™”: {json.dumps(wpm_timeline[:5])[:300]}...
        
        ğŸ“„ ì‘ë‹µì€ ì•„ë˜ êµ¬ì¡°ì˜ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

        - overallScore, clarity, speed, volume, confidence: 0~100 ì‚¬ì´ì˜ ì •ìˆ˜ (confidenceëŠ” overallScoreì™€ ë™ì¼)
        - speechMetrics: í•˜ìœ„ í•­ëª© í¬í•¨
            â€¢ wordsPerMinute, clarity, intonation, pauseDuration, pronunciation, fillers
            â€¢ ìˆ«ì ë˜ëŠ” ì†Œìˆ˜ 1ìë¦¬ (pauseDurationì€ ì´ˆ ë‹¨ìœ„)
        - metricGrades: ê° í•­ëª©ì— ëŒ€í•´ ì•„ë˜ í˜•ì‹
            {{ "grade": "ë“±ê¸‰", "comment": "ì§§ì€ ì„¤ëª…" }}
            â€¢ ë“±ê¸‰ ë²”ì£¼ëŠ” ê° í•­ëª©ì— ë”°ë¼ ë‹¬ë¼ì§ (ì•„ë˜ ì°¸ê³ )
            â€¢ wordsPerMinute, clarity, intonation, pauseDuration, pronunciation, fillers
        - voicePatterns:
            â€¢ volumePattern: {{ "description": ë¬¸ìì—´, "data": [ìˆ«ì ë°°ì—´] }}
            â€¢ speedPattern: {{ "description": ë¬¸ìì—´, "data": [{{"position": ì •ìˆ˜, "level": ì •ìˆ˜}}, ...] }}
            â€¢ tonePattern: {{ "description": ë¬¸ìì—´, "data": [{{"position": ì •ìˆ˜, "level": ì •ìˆ˜}}, ...] }}
        - interviewerComment: í•œë‘ ë¬¸ì¥ì˜ ë©´ì ‘ê´€ ì‹œì  í”¼ë“œë°±
        - strengths / improvements / improvementStrategies:
            â€¢ ê° í•­ëª©ì€ 4ê°œ ì´ìƒ ì‘ì„±
            â€¢ í˜•ì‹: {{ "title": "ì§§ì€ ì œëª©", "description": "..." }}
            â€¢ improvements ì™€ improvementStrategiesëŠ” 1:1 ëŒ€ì‘
        
        JSON í‚¤ ì´ë¦„ì€ ì •í™•í•˜ê²Œ ìœ ì§€í•´ì£¼ì„¸ìš”. ì£¼ì„ì´ë‚˜ ë¶ˆí•„ìš”í•œ í¬ë§·(ë°±í‹± ë“±)ì€ í¬í•¨í•˜ì§€ ë§ê³ , ê²°ê³¼ë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”.


        ì§€ì¹¨
        - ëª¨ë“  JSON ê²°ê³¼ëŠ” í•œêµ­ì–´ ë˜ëŠ” ìˆ«ìë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
        - ëª¨ë“  ì¶œë ¥ ë¬¸ì¥ì€ ë°˜ë“œì‹œ ~ìŠµë‹ˆë‹¤, ~í•©ë‹ˆë‹¤ì™€ ê°™ì€ ì¡´ëŒ“ë§ ì¢…ê²°ì–´ë¯¸ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.
        - ë°˜ë§, ëª…ë ¹í˜•, ìŒìŠ´ì²´ í‘œí˜„(~í•¨, ~ë¨, ~ì„, ~í•˜ì§€ ì•ŠìŒ ë“±)ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
        
        ì¶”ê°€ ì‘ì„± ê°€ì´ë“œ
        - JSON í‚¤Â·êµ¬ì¡° ë³€í˜• ê¸ˆì§€, ìˆ«ìëŠ” ì •ìˆ˜ ë˜ëŠ” ì†Œìˆ˜ 1ìë¦¬.
        - metricGrades ë“±ê¸‰ ê¸°ì¤€  
          â€¢ wordsPerMinute: <80=ëŠë¦¼, 80~100/130~160=ë‹¤ì†Œ, 100~130=ì ì ˆ, >160=ë¹ ë¦„  
          â€¢ clarity: â‰¥80=ìš°ìˆ˜, 60~79=ë³´í†µ, <60=ê°œì„  í•„ìš”  
          â€¢ pauseDuration: <0.5s=ë¹ ë¦„, 0.5~1.2s=ì ì ˆ, >1.2s=ê³¼ë‹¤  
          â€¢ fillers: 0~1=ìµœì†Œ, 2~4=ë³´í†µ, â‰¥5=ê³¼ë‹¤  
        - ìŒí–¥Â·ì–¸ì–´Â·ë…¼ë¦¬Â·íƒœë„ ë“± ë‹¤ê°ë„ë¡œ í‰ê°€.
        - ë™ì¼ ë²”ì£¼ë¼ë„ 'ì–´íˆ¬-ì¹œê·¼ê°' vs 'ì–´íˆ¬-ì „ë¬¸ì„±'ì²˜ëŸ¼ ì„¸ë¶€ ìš”ì†Œë¡œ ë¶„í™”.
        """

        gpt_resp = client.chat.completions.create(
            model="gpt-4.1-nano",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
        )

        # 4) GPT ì‘ë‹µ íŒŒì‹± ------------------------------------------------------
        cleaned = clean_md_json(gpt_resp.choices[0].message.content)
        parsed = json.loads(cleaned)          # dict

        # 5) tonePattern.data / speedPattern ì§ì ‘ ê³„ì‚° í›„ ì‚½ì… -------------------------------
        pitch_series = compute_pitch_contour(x, Fs, win, step)
        tone_pattern = extract_tone_pattern_from_pitch(pitch_series)

        if "voicePatterns" in parsed and "tonePattern" in parsed["voicePatterns"]:
            parsed["voicePatterns"]["tonePattern"]["data"] = tone_pattern
        else:
            parsed["voicePatterns"]["tonePattern"] = {
                "description": "ì‘ë‹µ ë‚´ tonePattern ëˆ„ë½ë¨, ì§ì ‘ ì¶”ê°€ë¨",
                "data": tone_pattern
            }

        if "voicePatterns" in parsed and "speedPattern" in parsed["voicePatterns"]:
            parsed["voicePatterns"]["speedPattern"]["data"] = speed_pattern_data
        else:
            parsed["voicePatterns"]["speedPattern"] = {
                "description": "ì‹œê°„ íë¦„ì— ë”°ë¥¸ ë§í•˜ê¸° ì†ë„ ë³€í™”",
                "data": speed_pattern_data
            }


        file_size = webm_path.stat().st_size # webm íŒŒì¼ ì‚¬ì´ì¦ˆ ê³„ì‚°

        end_time = time.perf_counter()  # ì¢…ë£Œ ì‹œê°„
        duration = round(end_time - start_time, 2)  # ê±¸ë¦° ì‹œê°„ (ì´ˆ)

        print('ê±¸ë¦°ì‹œê°„: ', duration)

        # 6) ìµœì¢… ë°˜í™˜ ----------------------------------------------------------
        return {
            "transcript": transcript,
            "gpt_feedback": parsed,
            "webmFn": str(webm_fn),
            "webmPath": str(webm_path),
            "wavPath": str(wav_path),
            "webmFileSize": file_size
        }

    except Exception as e:
        logging.exception("analyze_audio error")
        return {"error": str(e)}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# uvicorn main:app --reload
