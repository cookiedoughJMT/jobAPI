from fastapi import UploadFile, File
from fastapi import APIRouter
from dotenv import load_dotenv

import os, json, numpy as np

import whisper
from pyAudioAnalysis import audioBasicIO, ShortTermFeatures
from openai import OpenAI

from pydub import AudioSegment
from pydub.utils import which

import logging
import re

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê³µí†µ ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

ffmpeg_path = r"C:\ffmpeg\bin\ffmpeg.exe"

# í™˜ê²½ë³€ìˆ˜ ì¶”ê°€
os.environ["PATH"] += os.pathsep + os.path.dirname(ffmpeg_path)

# pydubì— ëª…ì‹œì  ì§€ì •
AudioSegment.converter = ffmpeg_path

# pydub ë‚´ë¶€ ê²½ë¡œì—ë„ ì„¤ì • (ì¶”ê°€ ë°©ì–´)
if not which("ffmpeg"):
    print("âŒ ffmpeg ê²½ë¡œ í™•ì¸ ì‹¤íŒ¨")
else:
    print("âœ… ffmpeg ê²½ë¡œ í™•ì¸ë¨:", which("ffmpeg"))

# FastAPI ì•±
audio_api = APIRouter()

# whisper ëª¨ë¸ (1íšŒë§Œ ë¡œë“œ)
whisper_model = whisper.load_model("base")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘  ìŒì„± ë¶„ì„ ë¼ìš°í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# audio_api = APIRouter(prefix="/api/audio")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í—¬í¼ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# pitch contour êµ¬í•˜ê¸° librosa(ìŒì„±ë¶„ì„ë¼ì´ë¸ŒëŸ¬ë¦¬)
def compute_pitch_contour(signal, fs, win, step):
    import librosa
    pitches, _ = librosa.piptrack(y=signal, sr=fs, n_fft=win, hop_length=step)
    pitch_contour = []
    for i in range(pitches.shape[1]):
        pitch_values = pitches[:, i]
        max_pitch = pitch_values.max()
        pitch_contour.append(max_pitch if max_pitch > 0 else 0)
    return pitch_contour

def extract_tone_pattern_from_pitch(pitch_series):
    result, length = [], len(pitch_series)
    step = max(1, length // 5)
    for i in range(0, length, step):
        segment = pitch_series[i:i + step]
        level = min(100, int(np.std(segment) * 1000))  # ë³€ë™ì„± â†’ 0~100
        result.append({"position": int(i / length * 100), "level": level})
    return result

def clean_md_json(text: str) -> str:
    """ ```json ... ```  ê°ì‹¸ê¸° ì œê±° """
    return re.sub(r"^```json\s*|\s*```$", "", text.strip(), flags=re.DOTALL)

def compute_wpm_timeline(words, segment_size=5.0):
    timeline = []
    current_start = 0.0
    end_time = words[-1]["end"] if words else 0

    print(words)

    while current_start < end_time:
        current_end = current_start + segment_size
        count = sum(1 for w in words if current_start <= w["start"] < current_end)
        wpm = int(count / segment_size * 60)
        timeline.append({"time": round(current_start), "wpm": wpm})
        current_start += segment_size

    return timeline

def convert_wpm_timeline_to_speed_pattern(wpm_timeline):
    if not wpm_timeline:
        return []

    end_time = wpm_timeline[-1]["time"] + 5.0  # ë§ˆì§€ë§‰ êµ¬ê°„ í¬í•¨

    pattern = []
    for item in wpm_timeline:
        position = int((item["time"] / end_time) * 100)  # 0~100%
        level = item["wpm"]  # ì •ê·œí™” ì—†ì´ ì‹¤ì œ wpm ê°’ ê·¸ëŒ€ë¡œ
        pattern.append({ "position": position, "level": level })

    return pattern

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@audio_api.post("/analyze")
async def analyze_audio(file: UploadFile = File(...)):
    try:
        # 0) ì—…ë¡œë“œ ì €ì¥ & ë³€í™˜ -------------------------------------------------
        raw = await file.read()
        with open("uploaded.webm", "wb") as f:
            f.write(raw)

        AudioSegment.from_file("uploaded.webm").export("uploaded.wav", format="wav")

        # 1) Whisper -----------------------------------------------------------
        transcript = whisper_model.transcribe("uploaded.wav")["text"]
        whisper_result = whisper_model.transcribe("uploaded.wav", word_timestamps=True, language='ko')

        # ë‹¨ì–´ ë‹¨ìœ„ ì‹œê°„ ì •ë³´ ì¶”ì¶œ
        words = []
        for seg in whisper_result.get("segments", []):
            words.extend(seg.get("words", []))

        # 2) pyAudioAnalysis ---------------------------------------------------
        Fs, x = audioBasicIO.read_audio_file("uploaded.wav")
        x = audioBasicIO.stereo_to_mono(x)

        # â­ï¸ librosa ì‚¬ìš©ì„ ìœ„í•´ float32ë¡œ ë³€í™˜
        x = x.astype(np.float32)

        if np.mean(x ** 2) < 1e-5:
            return {"error": "ë¬´ìŒì´ê±°ë‚˜ ë„ˆë¬´ ì‘ìŒ"}

        win = step = int(0.05 * Fs)
        F, f_names = ShortTermFeatures.feature_extraction(x, Fs, win, step)

        # print("ğŸ§ f_names í™•ì¸:", f_names)


        if F.shape[1] == 0:
            return {"error": "ìœ íš¨í•œ í”„ë ˆì„ ì—†ìŒ"}

        features = {k: float(np.mean(v)) for k, v in zip(f_names, F)}
        feat_txt = "\n".join(f"- {k}: {v:.4f}" for k, v in features.items())

        print('transcript: ', transcript)
        wpm_timeline = compute_wpm_timeline(words)
        speed_pattern_data = convert_wpm_timeline_to_speed_pattern(wpm_timeline)
        print('speed_pattern_data: ', speed_pattern_data)

        # 3) GPT í”„ë¡¬í”„íŠ¸ -------------------------------------------------------
        prompt = f"""
        ë‹¤ìŒì€ í•œ ì‚¬ìš©ìì˜ ìŒì„± ë©´ì ‘ ë°ì´í„°ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸ì™€ ìŒì„± í”¼ì²˜ë¥¼ ì°¸ê³ í•´ ì•„ë˜ JSON ìŠ¤í‚¤ë§ˆì— **ë”± ë§ì¶°ì„œ** (ë°±í‹±Â·ì£¼ì„ ì—†ì´) ì‘ë‹µí•˜ì„¸ìš”.

        â—ï¸í•µì‹¬ ì˜ë¬´ ì‚¬í•­
        1. strengths / improvements / improvementStrategies í•­ëª©ì— ìµœì†Œ 8ê°œì”©, ê°€ëŠ¥í•˜ë©´ 10ê°œ ì´ìƒ ì‘ì„±.
        2. ì œëª©(title)ì€ ì„œë¡œ ë‹¤ë¥¸ ê´€ì (ë…¼ë¦¬Â·ì „ë¬¸ì„±Â·ìì‹ ê°Â·ì–´ì¡°Â·ì†ë„Â·ë§íˆ¬Â·ê³µê°ë ¥Â·ì‹œê°„ ê´€ë¦¬ ë“±)ìœ¼ë¡œ ë‹¤ì–‘í™”.
        3. description ì—ëŠ” êµ¬ì²´ì  ì‚¬ë¡€ë¥¼ í¬í•¨.
        4. improvements ì™€ improvementStrategies ëŠ” 1:1ë¡œ ëŒ€ì‘(ê°™ì€ ìˆœì„œ).

        í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ê±°ë‚˜ ì„±ì˜ê°€ ë¶€ì¡±í•˜ë©´ confidence, overallScore ë¥¼ 10~30 ì˜ì—­ìœ¼ë¡œ ë‚®ì¶° ì£¼ì„¸ìš”.
                
        strengths í•­ëª©ì€ ìµœì†Œ 8ê°œ ì´ìƒ í•´ì£¼ì„¸ìš”.
        improvements í•­ëª©ì€ ìµœì†Œ 8ê°œ ì´ìƒ í•´ì£¼ì„¸ìš”.
        improvementStrategies í•­ëª©ì€ ìµœì†Œ 8ê°œ ì´ìƒ í•´ì£¼ì„¸ìš”.                
                
        ì¤‘ë³µ ê¸ˆì§€:
        - strengths.title ê³¼ improvements.title ì€ ì ˆëŒ€ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ ì‘ì„±í•˜ì„¸ìš”.
          (ì¤‘ë³µì´ ìƒê¸¸ ê²½ìš°, improvements ìª½ í•­ëª©ì„ ì œê±°)
        
        ê·¼ê±° í•„ìˆ˜:
        - ëª¨ë“  description ì—ì„œëŠ” ì‹¤ì œ ìŒì„±Â·í…ìŠ¤íŠ¸ì—ì„œ í™•ì¸ ê°€ëŠ¥í•œ êµ¬ì²´ì  ê·¼ê±°ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.
          ê·¼ê±° ì—†ì´ ì¶”ìƒì  ì¹­ì°¬(ì˜ˆ: â€˜ì„±ì‹¤í•¨â€™, â€˜ê³µê°ì„± ë†’ìŒâ€™)ì€ ê¸ˆì§€í•©ë‹ˆë‹¤.
          
        ë¬¸ë§¥ë§Œ ë‚œí•´í•  ê²½ìš°ì—” ì˜ë¯¸ë¥¼ ë³´ì •Â·ì¶”ë¡ í•´ ìµœëŒ€í•œ ë„ˆê·¸ëŸ½ê²Œ í‰ê°€í•©ë‹ˆë‹¤.

        ğŸ—£ í…ìŠ¤íŠ¸
        \"\"\"{transcript}\"\"\"

        ğŸ§ ìŒì„± í”¼ì²˜ ìš”ì•½
        {feat_txt}
        - êµ¬ê°„ë³„ ì†ë„ ë³€í™”: {json.dumps(wpm_timeline[:5])[:300]}...
        
        {{
          "overallScore":  (10~100 ì •ìˆ˜),
          "clarity":       (0~100 ì •ìˆ˜),
          "speed":         (0~100 ì •ìˆ˜),
          "volume":        (0~100 ì •ìˆ˜),
          "confidence":    (overallScore ë™ì¼),

          "speechMetrics": {{
            "wordsPerMinute": 0,
            "clarity":        0,
            "intonation":     0,
            "pauseDuration":  0.0,
            "pronunciation":  0,
            "fillers":        0
          }},

          "metricGrades": {{
            "wordsPerMinute": {{ "grade": "ì ì ˆ/ë¹ ë¦„/ëŠë¦¼", "comment": "ì§§ì€ ë¶„ì„" }},
            "clarity":        {{ "grade": "ìš°ìˆ˜/ë³´í†µ/ê°œì„  í•„ìš”", "comment": "ì§§ì€ ë¶„ì„" }},
            "intonation":     {{ "grade": "í’ë¶€/ë‹¨ì¡°ë¡œì›€", "comment": "ì§§ì€ ë¶„ì„" }},
            "pauseDuration":  {{ "grade": "ì ì ˆ/ë¹ ë¦„/ê³¼ë‹¤", "comment": "ì§§ì€ ë¶„ì„" }},
            "pronunciation":  {{ "grade": "ìš°ìˆ˜/ê°œì„  í•„ìš”", "comment": "ì§§ì€ ë¶„ì„" }},
            "fillers":        {{ "grade": "ìµœì†Œ/ë³´í†µ/ê³¼ë‹¤", "comment": "ì§§ì€ ë¶„ì„" }}
          }},

          "voicePatterns": {{
            "volumePattern": {{
              "description": "ë¬¸ì¥",
              "data": [0,0,0,0,0]
            }},
            "speedPattern": {{
              "description": "ë¬¸ì¥",
              "data": [{{"position":0,"level":0}},{{"position":10,"level":1}},...]
            }},
            "tonePattern": {{
              "description": "ë¬¸ì¥"
            }}
          }},

          "interviewerComment": "ë©´ì ‘ê´€ ì‹œì ì—ì„œ ëŠê»´ì§ˆ ì „ë°˜ì  ì¸ìƒ í•œë‘ ë¬¸ì¥",

          "strengths": [
            {{ "title": "ì§§ì€ ì œëª©(ê°•ì )", "description": "êµ¬ì²´ ì‚¬ë¡€ í¬í•¨" }}, ...
          ],
          "improvements": [
            {{ "title": "ì§§ì€ ì œëª©(ê°œì„ ì )", "description": "êµ¬ì²´ ì‚¬ë¡€ í¬í•¨" }}, ...
          ],
          "improvementStrategies": [
            {{ "title": "ì§§ì€ ì œëª©(ê°œì„  ì „ëµ)", "description": "ê°œì„ ì ê³¼ 1:1 ëŒ€ì‘ë˜ëŠ” ì‹¤í–‰ ê°€ëŠ¥í•œ ì „ëµ" }}, ...
          ]
        }}

        ì§€ì¹¨
        - ëª¨ë“  JSON ê²°ê³¼ëŠ” í•œêµ­ì–´ ë˜ëŠ” ìˆ«ìë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
        
        ì¶”ê°€ ì‘ì„± ê°€ì´ë“œ
        - JSON í‚¤Â·êµ¬ì¡° ë³€í˜• ê¸ˆì§€, ìˆ«ìëŠ” ì •ìˆ˜ ë˜ëŠ” ì†Œìˆ˜ 1ìë¦¬.
        - metricGrades ë“±ê¸‰ ê¸°ì¤€  
          â€¢ wordsPerMinute: <80=ëŠë¦¼, 80~100/130~160=ë‹¤ì†Œ, 100~130=ì ì ˆ, >160=ë¹ ë¦„  
          â€¢ clarity: â‰¥80=ìš°ìˆ˜, 60~79=ë³´í†µ, <60=ê°œì„  í•„ìš”  
          â€¢ pauseDuration: <0.5s=ë¹ ë¦„, 0.5~1.2s=ì ì ˆ, >1.2s=ê³¼ë‹¤  
          â€¢ fillers: 0~1=ìµœì†Œ, 2~4=ë³´í†µ, â‰¥5=ê³¼ë‹¤  
        - ìŒí–¥Â·ì–¸ì–´Â·ë…¼ë¦¬Â·íƒœë„ ë“± ë‹¤ê°ë„ë¡œ í‰ê°€.
        - ë™ì¼ ë²”ì£¼ë¼ë„ 'ì–´íˆ¬-ì¹œê·¼ê°' vs 'ì–´íˆ¬-ì „ë¬¸ì„±'ì²˜ëŸ¼ ì„¸ë¶€ ìš”ì†Œë¡œ ë¶„í™”.
        """

        gpt_resp = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
        )

        # 4) GPT ì‘ë‹µ íŒŒì‹± ------------------------------------------------------
        cleaned = clean_md_json(gpt_resp.choices[0].message.content)
        parsed = json.loads(cleaned)          # dict

        # 5) tonePattern.data ì§ì ‘ ê³„ì‚° í›„ ì‚½ì… -------------------------------
        pitch_series = compute_pitch_contour(x, Fs, win, step)
        tone_pattern = extract_tone_pattern_from_pitch(pitch_series)

        if "voicePatterns" in parsed and "tonePattern" in parsed["voicePatterns"]:
            parsed["voicePatterns"]["tonePattern"]["data"] = tone_pattern
        else:
            parsed["voicePatterns"]["tonePattern"] = {
                "description": "ì‘ë‹µ ë‚´ tonePattern ëˆ„ë½ë¨, ì§ì ‘ ì¶”ê°€ë¨",
                "data": tone_pattern
            }

        if "voicePatterns" in parsed and "speedPattern" in parsed["voicePatterns"]:
            parsed["voicePatterns"]["speedPattern"]["data"] = speed_pattern_data
        else:
            parsed["voicePatterns"]["speedPattern"] = {
                "description": "ì‹œê°„ íë¦„ì— ë”°ë¥¸ ë§í•˜ê¸° ì†ë„ ë³€í™”",
                "data": speed_pattern_data
            }

        # 6) ìµœì¢… ë°˜í™˜ ----------------------------------------------------------
        return {
            "transcript": transcript,
            "features": features,
            "gpt_feedback": parsed     # â† ì´ë¯¸ dict!
        }

    except Exception as e:
        logging.exception("analyze_audio error")
        return {"error": str(e)}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# uvicorn main:app --reload
