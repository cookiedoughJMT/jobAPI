import json
import logging
from typing import List, Dict, Any, Tuple
from sentence_transformers import SentenceTransformer, util
from openai import OpenAI
import numpy as np

from config import (
    OPENAI_API_KEY, 
    SIMILARITY_THRESHOLD, 
    SENTENCE_MODEL, 
    GPT_MODEL, 
    GPT_TEMPERATURE
)

logger = logging.getLogger(__name__)

class EssayAnalysisService:
    def __init__(self):
        """ìì†Œì„œ ë¶„ì„ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self.sentence_model = SentenceTransformer(SENTENCE_MODEL)
        self.openai_client = OpenAI(api_key=OPENAI_API_KEY)
        
    def group_similar_sentences(self, sentences: List[str]) -> List[List[int]]:
        """
        ì˜ë¯¸ ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì¥ë“¤ì„ ê·¸ë£¹í•‘
        
        Args:
            sentences: ë¶„ì„í•  ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[List[int]]: ê° ê·¸ë£¹ì˜ ë¬¸ì¥ ì¸ë±ìŠ¤ë“¤
        """
        if not sentences:
            return []
            
        logger.info(f"=== ë¬¸ì¥ ê·¸ë£¹í•‘ ì‹œì‘ ===")
        logger.info(f"ì „ì²´ ë¬¸ì¥ ìˆ˜: {len(sentences)}")
        logger.info(f"ìœ ì‚¬ë„ ì„ê³„ê°’: {SIMILARITY_THRESHOLD}")
        
        # ëª¨ë“  ë¬¸ì¥ ë¡œê·¸ ì¶œë ¥
        for i, sentence in enumerate(sentences):
            logger.info(f"ë¬¸ì¥ [{i}]: {sentence}")
            
        # ë¬¸ì¥ ì„ë² ë”© ìƒì„±
        logger.info("ë¬¸ì¥ ì„ë² ë”© ìƒì„± ì¤‘...")
        embeddings = self.sentence_model.encode(sentences, convert_to_tensor=True)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        logger.info("ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...")
        cosine_scores = util.cos_sim(embeddings, embeddings).cpu().numpy()
        
        # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ë¡œê·¸ ì¶œë ¥ (ì„ê³„ê°’ ì´ìƒì¸ ê²ƒë§Œ)
        logger.info("=== ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ (ì„ê³„ê°’ ì´ìƒë§Œ) ===")
        for i in range(len(sentences)):
            for j in range(i + 1, len(sentences)):
                similarity = cosine_scores[i][j]
                if similarity > SIMILARITY_THRESHOLD:
                    logger.info(f"ë¬¸ì¥ [{i}] â†” ë¬¸ì¥ [{j}]: ìœ ì‚¬ë„ {similarity:.4f} â­ (ì„ê³„ê°’ ì´ˆê³¼)")
                    logger.info(f"  [{i}]: {sentences[i][:50]}...")
                    logger.info(f"  [{j}]: {sentences[j][:50]}...")
        
        # ìœ ì‚¬ë„ ê¸°ë°˜ ê·¸ë£¹í•‘
        logger.info("=== ê·¸ë£¹í•‘ ì‹œì‘ ===")
        visited = set()
        groups = []
        
        for i in range(len(sentences)):
            if i in visited:
                continue
                
            group = [i]
            visited.add(i)
            similar_found = []
            
            for j in range(i + 1, len(sentences)):
                if j not in visited and cosine_scores[i][j] > SIMILARITY_THRESHOLD:
                    group.append(j)
                    visited.add(j)
                    similar_found.append((j, cosine_scores[i][j]))
                    
            groups.append(group)
            
            # ê·¸ë£¹ í˜•ì„± ë¡œê·¸
            if len(group) > 1:
                logger.info(f"ğŸ“Œ ê·¸ë£¹ {len(groups)} í˜•ì„±: ë¬¸ì¥ {group} (ì´ {len(group)}ê°œ)")
                logger.info(f"  ëŒ€í‘œ ë¬¸ì¥ [{i}]: {sentences[i]}")
                for j, sim in similar_found:
                    logger.info(f"  ìœ ì‚¬ ë¬¸ì¥ [{j}]: {sentences[j]} (ìœ ì‚¬ë„: {sim:.4f})")
            else:
                logger.info(f"ğŸ“Œ ë‹¨ë… ë¬¸ì¥: [{i}] {sentences[i][:50]}...")
                
        logger.info(f"=== ê·¸ë£¹í•‘ ì™„ë£Œ ===")
        logger.info(f"ì´ ê·¸ë£¹ ìˆ˜: {len(groups)}")
        logger.info(f"ìœ ì‚¬ ê·¸ë£¹ ìˆ˜: {len([g for g in groups if len(g) > 1])}")
        logger.info(f"ë‹¨ë… ë¬¸ì¥ ìˆ˜: {len([g for g in groups if len(g) == 1])}")
        
        # ìµœì¢… ê·¸ë£¹ ìš”ì•½
        similar_groups = [g for g in groups if len(g) > 1]
        single_groups = [g for g in groups if len(g) == 1]
        
        logger.info("=== ìµœì¢… ê·¸ë£¹ ìš”ì•½ ===")
        for i, group in enumerate(similar_groups, 1):
            logger.info(f"ìœ ì‚¬ ê·¸ë£¹ {i}: {len(group)}ê°œ ë¬¸ì¥ -> 1ê°œë¡œ ì¶•ì•½")
            for idx in group:
                logger.info(f"  ë¬¸ì¥ [{idx}]: {sentences[idx]}")
                
        logger.info(f"ë‹¨ë… ë¬¸ì¥: {len(single_groups)}ê°œ")
        total_original = len(sentences)
        total_after_grouping = len(similar_groups) + len(single_groups)
        removed_count = total_original - total_after_grouping
        logger.info(f"ë¬¸ì¥ ìˆ˜ ë³€í™”: {total_original}ê°œ -> {total_after_grouping}ê°œ (ì œê±°: {removed_count}ê°œ)")
            
        return groups
    
    def prepare_grouped_sentences_for_gpt(self, sentences: List[str], groups: List[List[int]]) -> List[Dict[str, Any]]:
        """
        GPTì—ê²Œ ì „ë‹¬í•  ê·¸ë£¹í™”ëœ ë¬¸ì¥ ë°ì´í„° ì¤€ë¹„
        
        Args:
            sentences: ì›ë³¸ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
            groups: ê·¸ë£¹í•‘ëœ ë¬¸ì¥ ì¸ë±ìŠ¤ë“¤
            
        Returns:
            List[Dict]: ê° ê·¸ë£¹ì˜ ë¬¸ì¥ë“¤ê³¼ ë©”íƒ€ë°ì´í„°
        """
        grouped_data = []
        
        for group_index, group in enumerate(groups):
            if group:  # ê·¸ë£¹ì´ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°
                group_sentences = [sentences[i] for i in group]
                grouped_data.append({
                    "group_id": group_index + 1,
                    "sentences": group_sentences,
                    "sentence_indices": group,
                    "is_similar_group": len(group) > 1  # ìœ ì‚¬í•œ ë¬¸ì¥ì´ 2ê°œ ì´ìƒì¸ ê·¸ë£¹
                })
        
        return grouped_data
    
    def build_gpt_prompt(self, grouped_data: List[Dict[str, Any]]) -> str:
        """
        GPT ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ê·¸ë£¹í™”ëœ ë¬¸ì¥ ê¸°ë°˜)
        
        Args:
            grouped_data: ê·¸ë£¹í™”ëœ ë¬¸ì¥ ë°ì´í„°
            
        Returns:
            str: êµ¬ì„±ëœ í”„ë¡¬í”„íŠ¸
        """
        # ê·¸ë£¹ë³„ ë¬¸ì¥ ì •ë³´ êµ¬ì„±
        similar_groups_text = ""
        single_sentences_text = ""
        
        for group in grouped_data:
            group_id = group["group_id"]
            sentences = group["sentences"]
            is_similar = group["is_similar_group"]
            
            if is_similar:
                similar_groups_text += f"\nğŸ“Œ ê·¸ë£¹ {group_id} (ì˜ë¯¸ ìœ ì‚¬í•œ ë¬¸ì¥ë“¤ - ìµœì  1ê°œë§Œ ì„ íƒ):\n"
                for i, sentence in enumerate(sentences, 1):
                    similar_groups_text += f"  {group_id}-{i}. {sentence}\n"
            else:
                single_sentences_text += f"\nğŸ“Œ ë‹¨ë… ë¬¸ì¥ {group_id}:\n"
                single_sentences_text += f"  - {sentences[0]}\n"
        
        prompt = f"""ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ìê¸°ì†Œê°œì„œ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ë‹¤ìŒ ë¬¸ì¥ë“¤ì„ ë¶„ì„í•˜ê³  ê°œì„ í•´ ì£¼ì„¸ìš”.

ğŸš¨ **í•µì‹¬ ì›ì¹™**: correctionsëŠ” ì˜¤ì§ ì•„ë˜ 2ê°€ì§€ ê²½ìš°ì—ë§Œ ìƒì„±í•˜ì„¸ìš”:
1. **ì¤‘ë³µ ì œê±°**: ì˜ë¯¸ ìœ ì‚¬ ê·¸ë£¹ì—ì„œ ì„ íƒë˜ì§€ ì•Šì€ ë¬¸ì¥ë“¤ (improved = "", reason = "'[ì„ íƒëœ ë¬¸ì¥ ì•ë¶€ë¶„]...'ì™€ ì˜ë¯¸ìƒ ì¤‘ë³µ ë‚´ìš©ì´ë¼ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤")
2. **ì‹¤ì œ ê°œì„  í•„ìš”**: ë§ì¶¤ë²•, ë¬¸ë²•, í‘œí˜„ì— ëª…í™•í•œ ì˜¤ë¥˜ê°€ ìˆëŠ” ë¬¸ì¥ë“¤ë§Œ

**ì ˆëŒ€ í•˜ì§€ ë§ˆì„¸ìš”**: ì´ë¯¸ ì™„ë²½í•œ ë¬¸ì¥ì„ correctionsì— í¬í•¨í•˜ëŠ” ê²ƒ

ğŸ“Œ ë¶„ì„ ë° ê°œì„  ê¸°ì¤€:
1. **ì˜ë¯¸ ìœ ì‚¬ ê·¸ë£¹**: ê° ê·¸ë£¹ì—ì„œ ê°€ì¥ ì¢‹ì€ ë¬¸ì¥ 1ê°œë§Œ ì„ íƒí•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” ì¤‘ë³µ ì œê±°

2. **ë¬¸ë²• ë§ì¶¤ë²• ê°œì„ ** (ì„ íƒëœ ë¬¸ì¥ + ë‹¨ë… ë¬¸ì¥):
   âœ… **ë§ì¶¤ë²• ë° ë„ì–´ì“°ê¸°**
   - ì² ì ì˜¤ë¥˜ ìˆ˜ì •
   - ì˜¬ë°”ë¥¸ ë„ì–´ì“°ê¸° ì ìš©  
   - í•œê¸€ ë§ì¶¤ë²• ê·œì¹™ ì¤€ìˆ˜
   
   âœ… **ë¬¸ë²• ë° ì–´ë²•**
   - ì£¼ì–´ì™€ ì„œìˆ ì–´ì˜ í˜¸ì‘ í™•ì¸
   - ì‹œì œì˜ ì¼ê´€ì„± ìœ ì§€
   - ì˜¬ë°”ë¥¸ ì–´ìˆœ ì ìš©
   - ì¡°ì‚¬ ì‚¬ìš©ë²• êµì •
   
   âœ… **ë¬¸ì²´ ë° í‘œí˜„**
   - ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì–´ì²´ë¡œ í†µì¼
   - ì ì ˆí•œ ê²½ì–´ë²• ì‚¬ìš©
   - ì¶•ì•½ì–´ë‚˜ ë¹„ì†ì–´ ì œê±°
   - ë¬¸ì¥ ê¸¸ì´ ì¡°ì ˆë¡œ ê°€ë…ì„± í–¥ìƒ

3. **ë¬¸ë§¥ ê³ ë ¤ ì—°ê²°ì„±**: ì•ë’¤ ë¬¸ì¥ê³¼ì˜ ìì—°ìŠ¤ëŸ¬ìš´ íë¦„ í™•ë³´
   - ì ì ˆí•œ ì ‘ì†ì–´ ì‚¬ìš©
   - ë…¼ë¦¬ì  ìˆœì„œì™€ êµ¬ì¡° ê³ ë ¤
   - ì „ì²´ ìì†Œì„œì˜ ì¼ê´€ì„± ìœ ì§€

4. **ë¶€ì¡±í•œ ì˜ì—­ ë¶„ì„**: ë‹¤ìŒ í•­ëª©ë“¤ì„ ì²´ê³„ì ìœ¼ë¡œ ê²€í† í•˜ì—¬ **ìì†Œì„œì— ì•„ì§ ì–¸ê¸‰ë˜ì§€ ì•Šì€** ë¶€ì¡±í•œ ì˜ì—­ë§Œ ë„ì¶œ
   
   âš ï¸ **ì¤‘ìš”**: 
   - ì´ë¯¸ ìì†Œì„œì— ì¶©ë¶„íˆ ì–¸ê¸‰ëœ ì˜ì—­ì€ ë¶€ì¡±í•œ ì˜ì—­ìœ¼ë¡œ ë¶„ë¥˜í•˜ì§€ ì•ŠìŒ
   - **ì–µì§€ë¡œ ë¶€ì¡±í•œ ì˜ì—­ì„ ë§Œë“¤ì§€ ë§ˆì„¸ìš”**: ëª¨ë“  ì˜ì—­ì´ ì¶©ë¶„íˆ ë‹¤ë¤„ì¡Œë‹¤ë©´ missing_areasëŠ” ë¹ˆ ë°°ì—´ []ë¡œ ë°˜í™˜
   - **ì™„ì„±ë„ ë†’ì€ ìì†Œì„œ**: 4ê°€ì§€ ì˜ì—­ì´ ëª¨ë‘ ì˜ ê°–ì¶°ì§„ ê²½ìš° ë¶€ì¡±í•œ ì˜ì—­ ì—†ìŒìœ¼ë¡œ íŒë‹¨
   
   âœ… **ì§ë¬´ ì—­ëŸ‰ í‘œí˜„**
   - ë¬¸ì œ ìƒí™©ì„ ì¸ì‹í•˜ê³  í•´ê²°í•œ ì‚¬ë¡€ì˜ ëª…í™•í•œ ì„œìˆ 
   - íŒ€ì›Œí¬, í˜‘ì—…, ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ëŠ¥ë ¥ì´ ë“œëŸ¬ë‚˜ëŠ” ê²½í—˜  
   - í•´ë‹¹ ì§ë¬´ì™€ ì—°ê´€ëœ ê¸°ìˆ ë ¥, ì „ê³µ, ìê²©ì¦, ì‹¤ë¬´ ê²½í—˜ì˜ ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„
   
   âœ… **ì„±ì‹¤ì„±ê³¼ íƒœë„**
   - ê¾¸ì¤€í•œ ë…¸ë ¥, ì¥ê¸°ê°„ ì§€ì†ëœ í™œë™, ìê¸°ê³„ë°œ ë…¸ë ¥ í‘œí˜„
   - ì™¸ë¶€ êµìœ¡, í•™ìŠµ, ìê²©ì¦ ì·¨ë“ ë“± ì„±ì‹¤í•œ ì¤€ë¹„ íƒœë„
   - ë§¡ì€ ì¼ì„ ì±…ì„ê° ìˆê²Œ ëê¹Œì§€ ìˆ˜í–‰í•œ ì‚¬ë¡€
   
   âœ… **ë¦¬ë”ì‹­ê³¼ ë„ì „ì •ì‹ **
   - ì¡°ì§ì„ ì´ëŒê±°ë‚˜ ì¡°ìœ¨í•œ ë¦¬ë” ê²½í—˜
   - ì‹¤íŒ¨ ê·¹ë³µì´ë‚˜ ìƒˆë¡œìš´ ì‹œë„ë¥¼ í•œ ë„ì „ ê²½í—˜
   - ê°ˆë“± í•´ê²° ë˜ëŠ” êµ¬ì„±ì› ê°„ ë¬¸ì œ ì¡°ì • ì‚¬ë¡€
   
   âœ… **ê²°ê³¼ ë° ì„±ê³¼ ì¤‘ì‹¬ í‘œí˜„**
   - ê²½í—˜ì˜ ê²°ê³¼ë¥¼ ìˆ˜ì¹˜ë‚˜ ì§€í‘œ(í¼ì„¼íŠ¸, ì¦ê°€ìœ¨, ìˆ˜ëŸ‰ ë“±)ë¡œ í‘œí˜„
   - íƒ€ì¸ì˜ í”¼ë“œë°±ì„ ìˆ˜ìš©í•˜ê³  ê°œì„ í•œ ì‚¬ë¡€
   - í™œë™ì´ë‚˜ í”„ë¡œì íŠ¸ì˜ ê²°ê³¼ì— ëŒ€í•œ íšŒê³  ë˜ëŠ” ë°˜ì„±

ğŸ“Œ ì‘ì—… ìˆœì„œ:
1. ì˜ë¯¸ ìœ ì‚¬ ê·¸ë£¹ì—ì„œ ìµœì  ë¬¸ì¥ ì„ íƒ (ë‚˜ë¨¸ì§€ëŠ” ì¤‘ë³µ ì œê±°ë¡œ correctionsì— ì¶”ê°€)
2. ì„ íƒëœ ë¬¸ì¥ë“¤ê³¼ ë‹¨ë… ë¬¸ì¥ë“¤ì„ **ì›ë³¸ ìˆœì„œëŒ€ë¡œ ë°°ì—´í•˜ì—¬ ì „ì²´ íë¦„ íŒŒì•…**
3. **ìì†Œì„œ ì „ì²´ ë‚´ìš© ë¶„ì„**: ì–¸ê¸‰ëœ ê²½í—˜, ê¸°ìˆ , ì „ê³µ, ê´€ì‹¬ì‚¬, í™œë™ ë“±ì„ íŒŒì•…
4. ê° ë¬¸ì¥ì„ ì²´ê³„ì ìœ¼ë¡œ ê°œì„  (ê°œì„ ì‚¬í•­ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ correctionsì— ì¶”ê°€)
   - ë§ì¶¤ë²•, ë„ì–´ì“°ê¸°, ë¬¸ë²• ì˜¤ë¥˜ ìˆ˜ì •
   - ë¬¸ì²´ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì–´ì²´ë¡œ í†µì¼
   - ì•ë’¤ ë¬¸ë§¥ì„ ê³ ë ¤í•œ ì—°ê²°ì„± í™•ë³´
5. **ì‹ ì¤‘í•œ ë¶€ì¡± ì˜ì—­ íŒë‹¨**: ì „ì²´ ìì†Œì„œë¥¼ ìœ„ 4ê°€ì§€ í•µì‹¬ ì˜ì—­ìœ¼ë¡œ ë¶„ì„
   - **ê° ì˜ì—­ì´ ì¶©ë¶„íˆ ë‹¤ë¤„ì§„ ê²½ìš°**: missing_areas = [] (ë¹ˆ ë°°ì—´)
   - **ì •ë§ ë¶€ì¡±í•œ ì˜ì—­ë§Œ**: ê°œì¸ ë§ì¶¤í˜• ê°œì„  ì œì•ˆ ì œê³µ (ì–µì§€ë¡œ ë§Œë“¤ì§€ ì•Šê¸°)

ğŸ“Œ ê°œì„  ì‹œ ì£¼ì˜ì‚¬í•­:
âš ï¸ **ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ ê²ƒ**
- ì›ë³¸ì˜ ì˜ë¯¸ì™€ í•µì‹¬ ë‚´ìš©
- ê°œì¸ì ì¸ ê²½í—˜ì´ë‚˜ êµ¬ì²´ì  ì‚¬ë¡€
- ì „ì²´ì ì¸ ë¬¸ë‹¨ êµ¬ì¡°ì™€ ìˆœì„œ
- ì§€ì›ìì˜ ê°œì„±ê³¼ íŠ¹ìƒ‰

âœ… **ê°œì„  ëŒ€ìƒ**
- ì–¸ì–´ì  ì˜¤ë¥˜ (ë§ì¶¤ë²•, ë¬¸ë²•, ë„ì–´ì“°ê¸°)
- ì–´ìƒ‰í•œ í‘œí˜„ê³¼ êµ¬ì–´ì²´
- ë¬¸ì¥ ê°„ ì—°ê²°ì„±ê³¼ íë¦„

ğŸ“Œ corrections ìƒì„± ê·œì¹™ (ì¤‘ìš”!):
- **ì¤‘ë³µ ì œê±°ë§Œ**: original = "ì œê±°ë  ë¬¸ì¥", improved = "", reason = "'[ì„ íƒëœ ë¬¸ì¥ì˜ ì•ë¶€ë¶„ 15-20ê¸€ì]...'ì™€ ì˜ë¯¸ìƒ ì¤‘ë³µ ë‚´ìš©ì´ë¼ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"
- **ì‹¤ì œ ê°œì„ ì´ í•„ìš”í•œ ê²½ìš°ë§Œ**: original = "ì›ë³¸ ë¬¸ì¥", improved = "ê°œì„ ëœ ë¬¸ì¥", reason = "ì‚¬ìš©ì ì¹œí™”ì ì´ê³  êµ¬ì²´ì ì¸ ê°œì„  ì´ìœ "
- **âš ï¸ ì™„ë²½í•œ ë¬¸ì¥ì€ correctionsì— ì ˆëŒ€ í¬í•¨í•˜ì§€ ì•ŠìŒ**: ë§ì¶¤ë²•, ë¬¸ë²•, í‘œí˜„ì´ ëª¨ë‘ ì˜¬ë°”ë¥¸ ë¬¸ì¥ì€ corrections ë°°ì—´ì—ì„œ ì œì™¸

ğŸ“Œ **ê°œì„  ì´ìœ  ì‘ì„± ê°€ì´ë“œ** (ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ):
- âŒ ë‚˜ìœ ì˜ˆ: "ë¬¸ë²• ì˜¤ë¥˜ ìˆ˜ì •"
- âœ… ì¢‹ì€ ì˜ˆ: "ë¬¸ì¥ì´ ë” ìì—°ìŠ¤ëŸ½ê²Œ ì½íˆë„ë¡ '~í•˜ì—¬'ë¡œ ì—°ê²°í–ˆìŠµë‹ˆë‹¤"
- âŒ ë‚˜ìœ ì˜ˆ: "ì–´ë²• ê°œì„ "  
- âœ… ì¢‹ì€ ì˜ˆ: "ì• ë¬¸ì¥ê³¼ì˜ ì—°ê²°ì„ ìœ„í•´ 'ë˜í•œ'ì„ ì¶”ê°€í•˜ì—¬ ê¸€ì˜ íë¦„ì„ ë§¤ë„ëŸ½ê²Œ í–ˆìŠµë‹ˆë‹¤"
- âŒ ë‚˜ìœ ì˜ˆ: "í‘œí˜„ ìˆ˜ì •"
- âœ… ì¢‹ì€ ì˜ˆ: "ë” êµ¬ì²´ì ì´ê³  ì„íŒ©íŠ¸ ìˆëŠ” í‘œí˜„ìœ¼ë¡œ ë°”ê¿” ì¸ì‚¬ë‹´ë‹¹ìì—ê²Œ ê°•í•œ ì¸ìƒì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤"

ğŸ“Œ **ì˜ˆì‹œ ë¬¸ì¥ ì‘ì„± ê°€ì´ë“œ** (missing_areasìš©):
- **âš ï¸ ì¤‘ë³µ ë°©ì§€ ì›ì¹™**: ìì†Œì„œì— ì´ë¯¸ ì–¸ê¸‰ëœ êµ¬ì²´ì ì¸ ê²½í—˜ì´ë‚˜ ë‚´ìš©ê³¼ ê²¹ì¹˜ì§€ ì•ŠëŠ” **ì™„ì „íˆ ìƒˆë¡œìš´** ì˜ˆì‹œë§Œ ì‘ì„±
- **ê°œì¸í™”ëœ ìƒˆë¡œìš´ ì˜ˆì‹œ ì‘ì„±**: ìì†Œì„œì— ì–¸ê¸‰ëœ ë¶„ì•¼/ê´€ì‹¬ì‚¬ë¥¼ ì°¸ê³ í•˜ë˜, ì•„ì§ ì–¸ê¸‰ë˜ì§€ ì•Šì€ **ë‹¤ë¥¸ ê²½í—˜**ìœ¼ë¡œ ì˜ˆì‹œ ì œê³µ

**ì¤‘ë³µ ë°©ì§€ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
1. ìì†Œì„œì— ì´ë¯¸ ì–¸ê¸‰ëœ êµ¬ì²´ì  ê²½í—˜(í”„ë¡œì íŠ¸, í™œë™, í•™ìŠµ ë“±)ê³¼ ë™ì¼í•˜ê±°ë‚˜ ìœ ì‚¬í•œ ë‚´ìš©ì¸ì§€ í™•ì¸
2. ì´ë¯¸ ì–¸ê¸‰ëœ ìˆ«ì(íŒ€ì› ìˆ˜, ê¸°ê°„, ì„±ê³¼ ë“±)ì™€ ê²¹ì¹˜ëŠ” ë‚´ìš©ì¸ì§€ í™•ì¸  
3. ë™ì¼í•œ ê¸°ìˆ /ë„êµ¬/ë°©ë²•ë¡ ì´ ì´ë¯¸ ì–¸ê¸‰ë˜ì—ˆëŠ”ì§€ í™•ì¸

**ì˜¬ë°”ë¥¸ ì˜ˆì‹œ ì‘ì„±ë²•:**
- âŒ ìì†Œì„œì— "Java ì›¹ í”„ë¡œì íŠ¸" ì–¸ê¸‰ â†’ "Java ê¸°ë°˜ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ..." (ì¤‘ë³µ!)
- âœ… ìì†Œì„œì— "Java ì›¹ í”„ë¡œì íŠ¸" ì–¸ê¸‰ â†’ "ì•Œê³ ë¦¬ì¦˜ ìŠ¤í„°ë””ë¥¼ 6ê°œì›”ê°„ ìš´ì˜í•˜ë©° ë§¤ì£¼ ë¬¸ì œ í•´ê²° ë°©ë²•ì„ ê³µìœ í–ˆìŠµë‹ˆë‹¤" (ìƒˆë¡œìš´ ê²½í—˜)
- âŒ ìì†Œì„œì— "5ëª… íŒ€ í”„ë¡œì íŠ¸" ì–¸ê¸‰ â†’ "5ëª…ì˜ íŒ€ì›ê³¼ í˜‘ì—…..." (ìˆ«ì ì¤‘ë³µ!)  
- âœ… ìì†Œì„œì— "5ëª… íŒ€ í”„ë¡œì íŠ¸" ì–¸ê¸‰ â†’ "ê°œì¸ í¬íŠ¸í´ë¦¬ì˜¤ í”„ë¡œì íŠ¸ 3ê°œë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì™„ì„±í•˜ì—¬ ì‹¤ë ¥ì„ ê²€ì¦ë°›ì•˜ìŠµë‹ˆë‹¤" (ë‹¤ë¥¸ ë°©ì‹ì˜ ê²½í—˜)

**ë¶„ì•¼ë³„ ìƒˆë¡œìš´ ì˜ˆì‹œ ì ‘ê·¼ë²•:**
- ê¸°ìˆ  ë¶„ì•¼: í”„ë¡œì íŠ¸ ê²½í—˜ ì–¸ê¸‰ ì‹œ â†’ ìŠ¤í„°ë””, ì˜¤í”ˆì†ŒìŠ¤ ê¸°ì—¬, ë¸”ë¡œê·¸ ìš´ì˜, ë©˜í† ë§ ë“± ë‹¤ë¥¸ í™œë™
- í˜‘ì—… ê²½í—˜: íŒ€ í”„ë¡œì íŠ¸ ì–¸ê¸‰ ì‹œ â†’ ë™ì•„ë¦¬ ìš´ì˜, íŠœí„°ë§, ë´‰ì‚¬í™œë™ ë“± ë‹¤ë¥¸ í˜‘ì—…  
- í•™ìŠµ ê²½í—˜: íŠ¹ì • ê¸°ìˆ  í•™ìŠµ ì–¸ê¸‰ ì‹œ â†’ ìê²©ì¦ ì·¨ë“, ê°•ì˜ ìˆ˜ê°•, ì„¸ë¯¸ë‚˜ ì°¸ì„ ë“± ë‹¤ë¥¸ í•™ìŠµ

ğŸ“Œ ë¬¸ë§¥ ê³ ë ¤ ê°œì„  ì˜ˆì‹œ:
- ì´ì „ ë¬¸ì¥ì´ "íŒ€ì›Œí¬ì˜ ì¤‘ìš”ì„±"ì„ ì–¸ê¸‰í–ˆë‹¤ë©´, ë‹¤ìŒ ë¬¸ì¥ì€ "ë˜í•œ", "ì´ì™€ ë”ë¶ˆì–´" ë“±ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°
- ê²°ë¡  ë¶€ë¶„ì—ì„œëŠ” "ë”°ë¼ì„œ", "ì´ëŸ¬í•œ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ" ë“±ì˜ í‘œí˜„ ì‚¬ìš©
- ì‹œê°„ ìˆœì„œë‚˜ ë…¼ë¦¬ì  ìˆœì„œì— ë§ëŠ” ì ‘ì†ì–´ì™€ í‘œí˜„ ì‚¬ìš©

ğŸ“Œ ë°˜í™˜ í˜•ì‹ (JSON):
{{
  "analysis": {{
    "corrections": [
      {{
        "original": "ì œê±°ë˜ê±°ë‚˜ ê°œì„ í•  ì›ë³¸ ë¬¸ì¥",
        "improved": "ê°œì„ ëœ ë¬¸ì¥ (ì¤‘ë³µ ì œê±°ì˜ ê²½ìš° ë¹ˆ ë¬¸ìì—´ '')",
        "reason": "ì¤‘ë³µ ì œê±° ì´ìœ  ë˜ëŠ” ë¬¸ë²•/í‘œí˜„ ê°œì„  ì´ìœ "
      }}
    ],
    "missing_areas": [
      {{
        "category": "ë¶€ì¡±í•œ ì˜ì—­ ì¹´í…Œê³ ë¦¬ (ì§ë¬´ ì—­ëŸ‰ í‘œí˜„/ì„±ì‹¤ì„±ê³¼ íƒœë„/ë¦¬ë”ì‹­ê³¼ ë„ì „ì •ì‹ /ê²°ê³¼ ë° ì„±ê³¼ ì¤‘ì‹¬ í‘œí˜„)",
        "description": "í•´ë‹¹ ì˜ì—­ì´ ë¶€ì¡±í•œ ì´ìœ ì™€ ì¤‘ìš”ì„±",
        "suggestions": [
          {{
            "title": "êµ¬ì²´ì ì¸ ê°œì„  ì œì•ˆ ì œëª©",
            "content": "ì–´ë–¤ ë‚´ìš©ì„ ì¶”ê°€í•´ì•¼ í•˜ëŠ”ì§€ ìƒì„¸ ì„¤ëª… (ìœ„ 4ê°€ì§€ í•µì‹¬ ì˜ì—­ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±)",
            "example": "ìì†Œì„œì— ì´ë¯¸ ì–¸ê¸‰ëœ ë‚´ìš©ê³¼ ê²¹ì¹˜ì§€ ì•ŠëŠ” ì™„ì „íˆ ìƒˆë¡œìš´ ê²½í—˜ì˜ ì˜ˆì‹œ ë¬¸ì¥ (ë”°ì˜´í‘œë‚˜ 'ì˜ˆë¥¼ ë“¤ì–´' ë“±ì˜ ì ‘ë‘ì‚¬ ì—†ì´ ì™„ì„±ëœ ë¬¸ì¥ë§Œ ì‘ì„±)",
            "insertion_point": {{
              "target_sentence": "ì´ ë¬¸ì¥ ë’¤ì— ì‚½ì…í•˜ë©´ ì¢‹ì„ ê¸°ì¡´ ë¬¸ì¥",
              "reason": "í•´ë‹¹ ìœ„ì¹˜ì— ì‚½ì…í•˜ëŠ” ì´ìœ "
            }}
          }}
        ]
      }}
    ]
    
    âš ï¸ **missing_areasê°€ ì—†ëŠ” ê²½ìš°**: "missing_areas": [] (ë¹ˆ ë°°ì—´ë¡œ ë°˜í™˜)
  }}
}}

âš ï¸ ì¤‘ìš” ê·œì¹™ (ë°˜ë“œì‹œ ì¤€ìˆ˜):
- ì˜ë¯¸ ìœ ì‚¬ ê·¸ë£¹ì—ì„œëŠ” ë°˜ë“œì‹œ 1ê°œ ë¬¸ì¥ë§Œ ì„ íƒí•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ì œê±°
- **ì¤‘ë³µ ì œê±°ë§Œ**: improved = "" (ë¹ˆ ë¬¸ìì—´), reason = "'[ì„ íƒëœ ë¬¸ì¥ì˜ ì•ë¶€ë¶„ 15-20ê¸€ì]...'ì™€ ì˜ë¯¸ìƒ ì¤‘ë³µ ë‚´ìš©ì´ë¼ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"
- **ì‹¤ì œ ê°œì„ ì´ í•„ìš”í•œ ê²½ìš°ë§Œ**: improved = "ê°œì„ ëœ ë¬¸ì¥", reason = "êµ¬ì²´ì  ê°œì„  ì´ìœ  (ë§ì¶¤ë²•/ë¬¸ë²•/í‘œí˜„/ì—°ê²°ì„±)"
- **ğŸš« ì ˆëŒ€ ê¸ˆì§€**: originalê³¼ improvedê°€ ì™„ì „íˆ ë™ì¼í•œ corrections ìƒì„± ê¸ˆì§€
- **ğŸš« ì ˆëŒ€ ê¸ˆì§€**: ì´ë¯¸ ì™„ë²½í•œ ë¬¸ì¥ì„ correctionsì— í¬í•¨í•˜ëŠ” ê²ƒ ê¸ˆì§€
- **ì˜¬ë°”ë¥¸ ì˜ˆì‹œ**: 10ê°œ ë¬¸ì¥ ì¤‘ 2ê°œë§Œ ì¤‘ë³µ ì œê±°, 3ê°œë§Œ ë¬¸ë²• ê°œì„ ì´ í•„ìš”í•˜ë‹¤ë©´ â†’ correctionsëŠ” 5ê°œë§Œ ìƒì„±

ğŸ“Œ ì¤‘ë³µ ì œê±° ì´ìœ  ì‘ì„± ê°€ì´ë“œ:
- ì„ íƒëœ ë¬¸ì¥ì˜ ì•ë¶€ë¶„ 15-20ê¸€ìë¥¼ ë”°ì˜´í‘œ ì•ˆì— í‘œì‹œ
- ì˜ˆì‹œ: "'íŒ€ í”„ë¡œì íŠ¸ì—ì„œ ë¦¬ë” ì—­í• ì„...'ì™€ ì˜ë¯¸ìƒ ì¤‘ë³µ ë‚´ìš©ì´ë¼ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"
- ì‚¬ìš©ìê°€ ì–´ë–¤ ë¬¸ì¥ì´ ì„ íƒë˜ì—ˆëŠ”ì§€ ë¹„êµí•  ìˆ˜ ìˆë„ë¡ ë„ì›€

ğŸ“Œ corrections ì˜¬ë°”ë¥¸ ì˜ˆì‹œ (ì‚¬ìš©ì ì¹œí™”ì  ì´ìœ  í¬í•¨):
```json
[
  {{
    "original": "ì €ëŠ” ëŠì„ì—†ì´ í•™ìŠµí•˜ë©° ë°œì „í•˜ëŠ” ê°œë°œìì…ë‹ˆë‹¤.",
    "improved": "",
    "reason": "'í•­ìƒ ìƒˆë¡œìš´ ê¸°ìˆ ì„ í•™ìŠµí•˜ê³  ì‹¤ë¬´ì— ì ìš©í•˜ëŠ”...'ì™€ ì˜ë¯¸ìƒ ì¤‘ë³µ ë‚´ìš©ì´ë¼ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"
  }},
  {{
    "original": "ìƒˆë¡œìš´ ê¸°ìˆ ì— ëŒ€í•œ í˜¸ê¸°ì‹¬ì´ ë§ê³ , ì´ë¥¼ ì‹¤ë¬´ì— ì ìš©í•˜ëŠ” ê²ƒì„ ì¢‹ì•„í•©ë‹ˆë‹¤.",
    "improved": "ë˜í•œ ìƒˆë¡œìš´ ê¸°ìˆ ì— ëŒ€í•œ í˜¸ê¸°ì‹¬ì´ ë§ìœ¼ë©°, ì´ë¥¼ ì‹¤ë¬´ì— ì ê·¹ì ìœ¼ë¡œ ì ìš©í•˜ì—¬ ì—…ë¬´ íš¨ìœ¨ì„±ì„ ë†’ì´ê³ ì í•©ë‹ˆë‹¤.",
    "reason": "ì• ë¬¸ì¥ê³¼ì˜ ì—°ê²°ì„ ìœ„í•´ 'ë˜í•œ'ì„ ì¶”ê°€í•˜ê³ , 'ì ê·¹ì ìœ¼ë¡œ', 'ì—…ë¬´ íš¨ìœ¨ì„±' ë“±ì˜ í‘œí˜„ìœ¼ë¡œ ë” êµ¬ì²´ì ì´ê³  ì „ë¬¸ì ì¸ ì¸ìƒì„ ì£¼ë„ë¡ ê°œì„ í–ˆìŠµë‹ˆë‹¤"
  }},
  {{
    "original": "ì €ëŠ” ì±…ì„ê°ì´ ê°•í•©ë‹ˆë‹¤. ë§¡ì€ ì—…ë¬´ë¥¼ ì™„ìˆ˜í•©ë‹ˆë‹¤.",
    "improved": "ì €ëŠ” ì±…ì„ê°ì´ ê°•í•˜ì—¬ ë§¡ì€ ì—…ë¬´ë¥¼ ëê¹Œì§€ ì™„ìˆ˜í•©ë‹ˆë‹¤.",
    "reason": "ì§§ê³  ë‹¨ì¡°ë¡œìš´ ë‘ ë¬¸ì¥ì„ '~í•˜ì—¬'ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•˜ê³ , 'ëê¹Œì§€'ë¥¼ ì¶”ê°€í•˜ì—¬ ì˜ì§€ê°€ ë” ê°•í•˜ê²Œ ëŠê»´ì§€ë„ë¡ í–ˆìŠµë‹ˆë‹¤"
  }},
  {{
    "original": "íŒ€ì›Œí¬ë¥¼ ì¤‘ìš”í•˜ê²Œ ìƒê°í•©ë‹ˆë‹¤.",
    "improved": "íŒ€ì›Œí¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ í˜‘ì—…ì„ ë§¤ìš° ì¤‘ìš”í•˜ê²Œ ìƒê°í•©ë‹ˆë‹¤.",
    "reason": "'í˜‘ì—…'ì´ë¼ëŠ” êµ¬ì²´ì  í‘œí˜„ì„ ì¶”ê°€í•˜ê³  'ë§¤ìš°'ë¡œ ê°•ì¡°í•˜ì—¬ íŒ€ì›Œí¬ì— ëŒ€í•œ ì§„ì •ì„±ì´ ë” ì˜ ë“œëŸ¬ë‚˜ë„ë¡ í–ˆìŠµë‹ˆë‹¤"
  }}
]
```

ğŸ“Œ ì˜ëª»ëœ ì˜ˆì‹œ (ìƒì„±í•˜ë©´ ì•ˆ ë¨):
```json
[
  {{
    "original": "ì €ëŠ” ì„±ì‹¤í•˜ê³  ì±…ì„ê° ìˆëŠ” ì‚¬ëŒì…ë‹ˆë‹¤.",
    "improved": "ì €ëŠ” ì„±ì‹¤í•˜ê³  ì±…ì„ê° ìˆëŠ” ì‚¬ëŒì…ë‹ˆë‹¤.",
    "reason": "ë¬¸ë²•ê³¼ í‘œí˜„ì´ ì ì ˆí•¨"
  }},
  {{
    "original": "íŒ€ì›Œí¬ë¥¼ ì¤‘ìš”í•˜ê²Œ ìƒê°í•©ë‹ˆë‹¤.",
    "improved": "íŒ€ì›Œí¬ë¥¼ ì¤‘ìš”í•˜ê²Œ ìƒê°í•©ë‹ˆë‹¤.",
    "reason": "ìˆ˜ì • ë¶ˆí•„ìš”"
  }}
]
```
â†’ â›” **ì´ëŸ° ê²½ìš°ëŠ” correctionsì— ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”!** 
â†’ âœ… **ëŒ€ì‹  ì™„ì „íˆ ë¬´ì‹œí•˜ê³  ë„˜ì–´ê°€ì„¸ìš”!**

ğŸ“Œ ì˜ë¯¸ ìœ ì‚¬ ê·¸ë£¹ë“¤ (ê° ê·¸ë£¹ì—ì„œ 1ê°œë§Œ ì„ íƒ):
{similar_groups_text}

ğŸ“Œ ë‹¨ë… ë¬¸ì¥ë“¤ (ë¬¸ë²•/í‘œí˜„ ê°œì„  ëŒ€ìƒ):
{single_sentences_text}
"""
        return prompt
    
    def call_gpt_analysis(self, grouped_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        GPTë¥¼ í˜¸ì¶œí•˜ì—¬ ìì†Œì„œ ë¶„ì„ ìˆ˜í–‰ (ê·¸ë£¹í™”ëœ ë¬¸ì¥ ê¸°ë°˜)
        
        Args:
            grouped_data: ê·¸ë£¹í™”ëœ ë¬¸ì¥ ë°ì´í„°
            
        Returns:
            Dict[str, Any]: GPT ë¶„ì„ ê²°ê³¼
        """
        try:
            prompt = self.build_gpt_prompt(grouped_data)
            
            response = self.openai_client.chat.completions.create(
                model=GPT_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=GPT_TEMPERATURE
            )
            
            result_text = response.choices[0].message.content
            
            # JSON íŒŒì‹± (markdown ì½”ë“œ ë¸”ë¡ ì œê±°)
            try:
                logger.info(f"GPT ì›ë³¸ ì‘ë‹µ: {result_text[:500]}...")  # ì‘ë‹µ ì¼ë¶€ ë¡œê¹…
                
                # markdown ì½”ë“œ ë¸”ë¡ ì œê±°
                clean_text = result_text.strip()
                if clean_text.startswith("```json"):
                    clean_text = clean_text[7:]  # ```json ì œê±°
                if clean_text.startswith("```"):
                    clean_text = clean_text[3:]  # ``` ì œê±°
                if clean_text.endswith("```"):
                    clean_text = clean_text[:-3]  # ëì˜ ``` ì œê±°
                clean_text = clean_text.strip()
                
                result = json.loads(clean_text)
                return result
            except json.JSONDecodeError as e:
                logger.error(f"GPT ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                logger.error(f"ì •ë¦¬ëœ í…ìŠ¤íŠ¸: {clean_text[:500]}...")
                return {
                    "analysis": {
                        "corrections": [],
                        "missing_areas": []
                    }
                }
                
        except Exception as e:
            logger.error(f"GPT í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise Exception(f"GPT ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    def analyze_essay(self, original_sentences: List[str]) -> Dict[str, Any]:
        """
        ìì†Œì„œ ì „ì²´ ë¶„ì„ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
        
        Args:
            original_sentences: ì›ë³¸ ë¬¸ì¥ë“¤
            
        Returns:
            Dict[str, Any]: ë¶„ì„ ê²°ê³¼
        """
        try:
            # 1. ì˜ë¯¸ ìœ ì‚¬ ê·¸ë£¹í•‘
            groups = self.group_similar_sentences(original_sentences)
            
            # 2. ê·¸ë£¹í™”ëœ ë°ì´í„° ì¤€ë¹„
            grouped_data = self.prepare_grouped_sentences_for_gpt(original_sentences, groups)
            
            # 3. GPT ë¶„ì„ í˜¸ì¶œ (ê·¸ë£¹ë³„ ìµœì  ë¬¸ì¥ ì„ íƒ + ë¶„ì„)
            gpt_result = self.call_gpt_analysis(grouped_data)
            
            # 4. ê²°ê³¼ êµ¬ì„±
            result = {
                "original_sentences_count": len(original_sentences),
                "groups": groups,
                "grouped_data": grouped_data,
                **gpt_result
            }
            
            return result
            
        except Exception as e:
            logger.error(f"ìì†Œì„œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise Exception(f"ìì†Œì„œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}") 